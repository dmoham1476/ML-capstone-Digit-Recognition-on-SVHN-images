{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Machine Learning Nanodegree Capstone Project\n",
    "=============\n",
    "\n",
    "This is my final model for the digit recognition from SVHN images project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (45000, 32, 32) (45000,)\n",
      "Validation set (6000, 32, 32) (6000,)\n",
      "Test set (15000, 32, 32) (15000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'SVHN.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (45000, 32, 32, 1) (45000, 10)\n",
      "Validation set (6000, 32, 32, 1) (6000, 10)\n",
      "Test set (15000, 32, 32, 1) (15000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's add another convolutional layer, followed by one fully connected layer. Experimenting with batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 16)\n",
      "(32, 14, 14, 16)\n",
      "(32, 10, 10, 16)\n",
      "(32, 5, 5, 16)\n",
      "(32, 1, 1, 128)\n",
      "Tensor(\"Reshape:0\", shape=(32, 128), dtype=float32)\n",
      "(6000, 28, 28, 16)\n",
      "(6000, 14, 14, 16)\n",
      "(6000, 10, 10, 16)\n",
      "(6000, 5, 5, 16)\n",
      "(6000, 1, 1, 128)\n",
      "Tensor(\"Reshape_1:0\", shape=(6000, 128), dtype=float32)\n",
      "(15000, 28, 28, 16)\n",
      "(15000, 14, 14, 16)\n",
      "(15000, 10, 10, 16)\n",
      "(15000, 5, 5, 16)\n",
      "(15000, 1, 1, 128)\n",
      "Tensor(\"Reshape_2:0\", shape=(15000, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 128\n",
    "#beta_regul = 1e-3\n",
    "drop_out = 0.95\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  \n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob):\n",
    "    # C1 input 32 x 32 x 1\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    print(bias1.get_shape())\n",
    "    # S2 input 28 x 28 x 16\n",
    "    pool2 = tf.nn.avg_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    print(pool2.get_shape())\n",
    "    # C3 input 14 x 14 x 16\n",
    "    conv3 = tf.nn.conv2d(pool2, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "    print(bias3.get_shape())\n",
    "    # S4 input 10 x 10 x 16\n",
    "    pool4 = tf.nn.avg_pool(bias3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    print(pool4.get_shape())\n",
    "    # C5 input 5 x 5 x 16\n",
    "    conv5 = tf.nn.conv2d(pool4, layer3_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias5 = tf.nn.relu(conv5 + layer3_biases)\n",
    "    print(bias5.get_shape())\n",
    "    # Dropout\n",
    "    hidden = tf.nn.dropout(bias5, keep_prob)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print(reshape)\n",
    "    #F6 input 1x1x128\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "    return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, drop_out)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.95, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "  #optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset, 1.0))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.123059\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 500: 0.795039\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 1000: 0.300794\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1500: 0.584539\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2000: 0.113191\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 2500: 0.269192\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3000: 0.272185\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3500: 0.532803\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 4000: 0.046885\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 4500: 0.101700\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 5000: 0.107931\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 5500: 0.645601\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 6000: 0.047778\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6500: 0.049165\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 7000: 0.018315\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7500: 0.332224\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 8000: 0.063772\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 8500: 0.279789\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 9000: 0.076587\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 9500: 0.111633\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 10000: 0.084844\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 10500: 0.077970\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 11000: 0.036493\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 11500: 0.017689\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 12000: 0.032738\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 12500: 0.015113\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 13000: 0.034195\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 13500: 0.103947\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 14000: 0.021458\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 14500: 0.007951\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 15000: 0.010587\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 15500: 0.183224\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 16000: 0.019332\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 16500: 0.058645\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 17000: 0.010374\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 17500: 0.176283\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 18000: 0.022715\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 18500: 0.033217\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 19000: 0.003516\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 19500: 0.077622\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 20000: 0.030003\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 20500: 0.054052\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 21000: 0.061169\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 21500: 0.061309\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 22000: 0.077280\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 22500: 0.055457\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 23000: 0.012104\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 23500: 0.007476\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 24000: 0.003997\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 24500: 0.049617\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 25000: 0.016507\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 25500: 0.102495\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 26000: 0.008987\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 26500: 0.068769\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 27000: 0.018151\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 27500: 0.005606\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 28000: 0.012676\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 28500: 0.004172\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 29000: 0.001417\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 29500: 0.002921\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 30000: 0.018612\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 30500: 0.014858\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 31000: 0.036534\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 31500: 0.003483\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 32000: 0.061227\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 32500: 0.029094\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 33000: 0.012175\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 33500: 0.005540\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 34000: 0.007286\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 34500: 0.006555\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 35000: 0.000750\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 35500: 0.032060\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 36000: 0.004355\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 36500: 0.007575\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 37000: 0.000568\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 37500: 0.143792\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 38000: 0.003826\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 38500: 0.004548\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 39000: 0.003341\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 39500: 0.006337\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 40000: 0.000880\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 40500: 0.006626\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 41000: 0.000295\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 41500: 0.008881\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 42000: 0.000283\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 42500: 0.012878\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 43000: 0.003440\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 43500: 0.026077\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 44000: 0.004841\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 44500: 0.000872\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 45000: 0.000263\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 45500: 0.025309\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 46000: 0.002323\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 46500: 0.002184\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 47000: 0.013623\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 47500: 0.046466\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 48000: 0.000795\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 48500: 0.005405\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 49000: 0.001315\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 49500: 0.008538\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 50000: 0.023098\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 50500: 0.018869\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 51000: 0.046279\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 51500: 0.005022\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 52000: 0.021820\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 52500: 0.004730\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 53000: 0.006895\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 53500: 0.009719\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 54000: 0.003897\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 54500: 0.121818\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 55000: 0.012262\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 55500: 0.004577\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 56000: 0.016492\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 56500: 0.004784\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 57000: 0.001307\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 57500: 0.017288\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 58000: 0.006361\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 58500: 0.002747\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 59000: 0.001529\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 59500: 0.006541\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 60000: 0.001921\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 60500: 0.011415\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 61000: 0.062263\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 61500: 0.034343\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 62000: 0.025087\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 62500: 0.022254\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 63000: 0.021095\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 63500: 0.005407\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 64000: 0.037338\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 64500: 0.003272\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 65000: 0.003273\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 65500: 0.002442\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 66000: 0.005037\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 66500: 0.003529\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 67000: 0.011844\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 67500: 0.000742\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 68000: 0.000581\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 68500: 0.002354\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 69000: 0.005743\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 69500: 0.008909\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 70000: 0.001046\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 70500: 0.003361\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 71000: 0.000081\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 71500: 0.003719\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 72000: 0.004318\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 72500: 0.043612\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 73000: 0.001076\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 73500: 0.000916\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 74000: 0.000530\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 74500: 0.024241\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 75000: 0.112177\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 75500: 0.001069\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 76000: 0.025907\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 76500: 0.011966\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 77000: 0.001770\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 77500: 0.011938\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 78000: 0.000359\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 78500: 0.000294\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 79000: 0.003687\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 79500: 0.001232\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 80000: 0.000607\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Test accuracy: 89.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 80001\n",
    "accuracy_plot = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      minibatch_accuracy = accuracy(predictions, batch_labels)\n",
    "      validation_accuracy = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      accuracy_plot.append((minibatch_accuracy,validation_accuracy,l))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 28, 28, 16)\n",
      "(16, 14, 14, 16)\n",
      "(16, 10, 10, 16)\n",
      "(16, 5, 5, 16)\n",
      "(16, 1, 1, 128)\n",
      "Tensor(\"Reshape:0\", shape=(16, 128), dtype=float32)\n",
      "(6000, 28, 28, 16)\n",
      "(6000, 14, 14, 16)\n",
      "(6000, 10, 10, 16)\n",
      "(6000, 5, 5, 16)\n",
      "(6000, 1, 1, 128)\n",
      "Tensor(\"Reshape_1:0\", shape=(6000, 128), dtype=float32)\n",
      "(15000, 28, 28, 16)\n",
      "(15000, 14, 14, 16)\n",
      "(15000, 10, 10, 16)\n",
      "(15000, 5, 5, 16)\n",
      "(15000, 1, 1, 128)\n",
      "Tensor(\"Reshape_2:0\", shape=(15000, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 128\n",
    "#beta_regul = 1e-3\n",
    "drop_out = 0.95\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  \n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob):\n",
    "    # C1 input 32 x 32 x 1\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    print(bias1.get_shape())\n",
    "    # S2 input 28 x 28 x 16\n",
    "    pool2 = tf.nn.avg_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    print(pool2.get_shape())\n",
    "    # C3 input 14 x 14 x 16\n",
    "    conv3 = tf.nn.conv2d(pool2, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "    print(bias3.get_shape())\n",
    "    # S4 input 10 x 10 x 16\n",
    "    pool4 = tf.nn.avg_pool(bias3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    print(pool4.get_shape())\n",
    "    # C5 input 5 x 5 x 16\n",
    "    conv5 = tf.nn.conv2d(pool4, layer3_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias5 = tf.nn.relu(conv5 + layer3_biases)\n",
    "    print(bias5.get_shape())\n",
    "    # Dropout\n",
    "    hidden = tf.nn.dropout(bias5, keep_prob)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print(reshape)\n",
    "    #F6 input 1x1x128\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "    return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, drop_out)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.95, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "  #optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset, 1.0))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.343128\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.4%\n",
      "Minibatch loss at step 500: 0.549044\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 1000: 1.164681\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 0.710528\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 2000: 0.368301\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2500: 0.667753\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 3000: 0.277265\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 3500: 0.346974\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 4000: 0.176623\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 4500: 0.119880\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 5000: 0.292992\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 5500: 0.626634\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 6000: 0.255107\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 6500: 0.011422\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 7000: 0.387096\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 7500: 0.207163\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8000: 0.012632\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8500: 0.196152\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 9000: 0.030909\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 9500: 0.166156\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 10000: 0.011282\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 10500: 0.031729\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 11000: 0.031377\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 11500: 0.015509\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 12000: 0.014382\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 12500: 0.045310\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13000: 0.038005\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 13500: 0.050764\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 14000: 0.118266\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 14500: 0.022865\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 15000: 0.059329\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 15500: 0.344948\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 16000: 0.009094\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 16500: 0.079118\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 17000: 0.069720\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 17500: 0.013658\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 18000: 0.007027\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 18500: 0.012695\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 19000: 0.004590\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 19500: 0.074106\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 20000: 0.027418\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 20500: 0.070408\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 21000: 0.094267\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 21500: 0.045633\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 22000: 0.105694\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 22500: 0.143436\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 23000: 0.006619\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 23500: 0.024576\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 24000: 0.068973\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 24500: 0.024437\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 25000: 0.231601\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 25500: 0.005140\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 26000: 0.066965\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 26500: 0.000237\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 27000: 0.087284\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 27500: 0.109993\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 28000: 0.057146\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 28500: 0.012963\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 29000: 0.075149\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 29500: 0.130977\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 30000: 0.015973\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 30500: 0.034568\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 31000: 0.005856\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 31500: 0.040043\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 32000: 0.044913\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 32500: 0.003502\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 33000: 0.081116\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 33500: 0.046126\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 34000: 0.084999\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 34500: 0.044159\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 35000: 0.005596\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 35500: 0.007222\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 36000: 0.000457\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 36500: 0.048230\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 37000: 0.010099\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 37500: 0.006016\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 38000: 0.000528\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 38500: 0.002706\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 39000: 0.025385\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 39500: 0.009014\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 40000: 0.019303\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 40500: 0.033473\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 41000: 0.118718\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 41500: 0.009568\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 42000: 0.016624\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 42500: 0.013253\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 43000: 0.012973\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 43500: 0.063408\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 44000: 0.037617\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 44500: 0.059310\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 45000: 0.039647\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 45500: 0.076926\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 46000: 0.025612\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 46500: 0.008770\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 47000: 0.023157\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 47500: 0.005147\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 48000: 0.009512\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 48500: 0.066956\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 49000: 0.007974\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 49500: 0.075188\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 50000: 0.017498\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 50500: 0.000141\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 51000: 0.011783\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 51500: 0.190726\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 52000: 0.031748\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 52500: 0.015251\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 53000: 0.005894\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 53500: 0.001428\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 54000: 0.007209\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 54500: 0.085355\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 55000: 0.013630\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 55500: 0.036031\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 56000: 0.001906\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 56500: 0.024729\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 57000: 0.003649\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 57500: 0.001182\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 58000: 0.004415\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 58500: 0.028636\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 59000: 0.004836\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 59500: 0.018469\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 60000: 0.027344\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 60500: 0.001868\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 61000: 0.090748\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 61500: 0.000732\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 62000: 0.102538\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 62500: 0.110054\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 63000: 0.000946\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 63500: 0.014349\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 64000: 0.000009\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 64500: 0.016770\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 65000: 0.042770\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 65500: 0.056488\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 66000: 0.000868\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 66500: 0.000023\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 67000: 0.000159\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 67500: 0.008689\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 68000: 0.000792\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 68500: 0.080557\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 69000: 0.052978\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 69500: 0.018594\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 70000: 0.003537\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 70500: 0.097828\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 71000: 0.005166\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 71500: 0.008720\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 72000: 0.009589\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 72500: 0.001181\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 73000: 0.029906\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 73500: 0.018281\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 74000: 0.084236\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 74500: 0.013510\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 75000: 0.016239\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 75500: 0.030925\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 76000: 0.000348\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 76500: 0.038213\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 77000: 0.005292\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 77500: 0.038725\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 78000: 0.000808\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 78500: 0.011768\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 79000: 0.031800\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 79500: 0.004702\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 80000: 0.000579\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 80500: 0.001497\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 81000: 0.006908\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 81500: 0.000238\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 82000: 0.016624\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 82500: 0.000046\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 83000: 0.000710\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 83500: 0.008503\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 84000: 0.012012\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 84500: 0.000864\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 85000: 0.001192\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 85500: 0.015335\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 86000: 0.034978\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 86500: 0.002625\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 87000: 0.001737\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 87500: 0.011036\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 88000: 0.011643\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 88500: 0.002156\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 89000: 0.016176\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 89500: 0.003333\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 90000: 0.001948\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 90500: 0.001012\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 91000: 0.005658\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 91500: 0.006015\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 92000: 0.001238\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 92500: 0.300151\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 93000: 0.001270\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 93500: 0.003172\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 94000: 0.000760\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 94500: 0.021878\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 95000: 0.000104\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 95500: 0.029530\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 96000: 0.001838\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 96500: 0.073996\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 97000: 0.027657\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 97500: 0.042847\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 98000: 0.001439\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 98500: 0.000834\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 99000: 0.007593\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 99500: 0.004046\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 100000: 0.002179\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Test accuracy: 90.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "accuracy_plot = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      minibatch_accuracy = accuracy(predictions, batch_labels)\n",
    "      validation_accuracy = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      accuracy_plot.append((minibatch_accuracy,validation_accuracy,l))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value of validation accuracy is 89.2078772803\n",
      "Max value of validation accuracy is 90.4833333333\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgYAAAGJCAYAAADxMfswAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns6b/ASTqmMpdiFEZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd829955910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = np.arange(201)*500\n",
    "\n",
    "mini_acc = map(lambda t: t[0], accuracy_plot)\n",
    "valid_acc = map(lambda t: t[1], accuracy_plot)\n",
    "mean = np.mean(valid_acc)\n",
    "max = np.max(valid_acc)\n",
    "\n",
    "print('Mean value of validation accuracy is', mean)\n",
    "print('Max value of validation accuracy is', max)\n",
    "\n",
    "plt.figure(figsize=(8,6), dpi=75)\n",
    "plt.scatter(steps, mini_acc)\n",
    "plt.scatter(steps, valid_acc, c='r')\n",
    "\n",
    "plt.title('Accuracy Over Num of Steps')\n",
    "plt.legend(['mini batch accuracy', 'validation accuray'])\n",
    "plt.xlabel('Number of Steps')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0838742 9.26075e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAGJCAYAAAAT7eBJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8314dccd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = np.arange(201)*500\n",
    "\n",
    "batch_loss = map(lambda t: t[2], accuracy_plot)\n",
    "print(np.mean(batch_loss), np.min(batch_loss))\n",
    "\n",
    "plt.figure(figsize=(8,6), dpi=75)\n",
    "plt.scatter(steps, batch_loss)\n",
    "\n",
    "plt.title('Loss Over Num of Steps')\n",
    "plt.xlabel('Number of Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
